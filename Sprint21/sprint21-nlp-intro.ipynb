{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['挫折を', '折を経', 'を経験', '経験し', '験した', 'した事', 'た事が', '事がな', 'がない', 'ない者', 'い者は', '者は、', 'は、何', '、何も', '何も新', 'も新し', '新しい', 'しい事', 'い事に', '事に挑', 'に挑戦', '挑戦し', '戦した', 'したこ', 'たこと', 'ことが', 'とが無', 'が無い', '無いと', 'いとい', 'という', 'いうこ', 'うこと', 'ことだ', 'とだ。']\n"
     ]
    }
   ],
   "source": [
    "def ngram(nlp, hoge):\n",
    "    box = []\n",
    "    for i in range(0, len(hoge)-nlp+1):\n",
    "        box.append(hoge[i:nlp+i])\n",
    "    return box\n",
    "\n",
    "hoge = \"挫折を経験した事がない者は、何も新しい事に挑戦したことが無いということだ。\"\n",
    "words_hoge = hoge.split(\" \")\n",
    "\n",
    "box = ngram(3, hoge)\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$$$$$$$$$挫\n",
      "$$$$$$$$挫折\n",
      "$$$$$$$挫折を\n",
      "$$$$$$挫折を経\n",
      "$$$$$挫折を経験\n",
      "$$$$挫折を経験し\n",
      "$$$挫折を経験した\n",
      "$$挫折を経験した事\n",
      "$挫折を経験した事が\n",
      "挫折を経験した事がな\n",
      "折を経験した事がない\n",
      "を経験した事がない者\n",
      "経験した事がない者は\n",
      "験した事がない者は、\n",
      "した事がない者は、何\n",
      "た事がない者は、何も\n",
      "事がない者は、何も新\n",
      "がない者は、何も新し\n",
      "ない者は、何も新しい\n",
      "い者は、何も新しい事\n",
      "者は、何も新しい事に\n",
      "は、何も新しい事に挑\n",
      "、何も新しい事に挑戦\n",
      "何も新しい事に挑戦し\n",
      "も新しい事に挑戦した\n",
      "新しい事に挑戦したこ\n",
      "しい事に挑戦したこと\n",
      "い事に挑戦したことが\n",
      "事に挑戦したことが無\n",
      "に挑戦したことが無い\n",
      "挑戦したことが無いと\n",
      "戦したことが無いとい\n",
      "したことが無いという\n",
      "たことが無いというこ\n",
      "ことが無いということ\n",
      "とが無いということだ\n",
      "が無いということだ。\n",
      "無いということだ。$\n",
      "いということだ。$$\n",
      "ということだ。$$$\n",
      "いうことだ。$$$$\n",
      "うことだ。$$$$$\n",
      "ことだ。$$$$$$\n",
      "とだ。$$$$$$$\n",
      "だ。$$$$$$$$\n",
      "。$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "tweet = \"SHOWROOM「こみの部屋」ありがとうございました〜??チームKについてたくさんお話できたかな！舞台、プロレスと今年も一緒に夏を過ごしそうです??? https://t.co/qxc047jzLg\"\n",
    "\n",
    "import ngram\n",
    "index = ngram.NGram(N=10)\n",
    "\n",
    "for two_gram in index.ngrams(index.pad(hoge)):\n",
    "    print(two_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. word2vec ツールを使用して分散表現を取得\n",
    "GitHubに公開されている word2vec をツールを使用して分散表現を取得する。<br>\n",
    "入力する文章は<br>\n",
    "\n",
    "I love Tom and I love Mike .<br>\n",
    "\n",
    "である。今回は window=2 size=5 で実行した。この文章の分散表現は以下の通りである。<br>\n",
    "\n",
    "1 5<br>\n",
    "</s> 0.080054 0.088388 -0.076605 -0.065561 0.027332 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1自然言語処理の分野ではどのような応用事例があるか\n",
    "### 日本語入力（かな漢字文字変換）\n",
    "キーボードで入力した「かな文字」を「漢字混じり文字」に変換する際、自然言語処理が使用される。ワープロの時代から研究されている分野で、最近は長い文章も精度良く変換できるようになっている。<br>\n",
    "\n",
    "事例\n",
    "- [ATOK](http://atok.com/)\n",
    "- [Google 日本語入力](https://www.google.co.jp/ime/)\n",
    "- [MicrosoftIME](https://ja.wikipedia.org/wiki/Microsoft_IME)\n",
    "\n",
    "### 機械翻訳\n",
    "Google翻訳をはじめ、ここ数年で急速に精度が上がってきている分野。また合成音声と組み合わせたサービスも一般的になってきている。\n",
    "\n",
    "事例\n",
    "\n",
    "- [Google翻訳](https://translate.google.com/)\n",
    "- [音声翻訳アプリ](https://www.nict.go.jp/data/app/voice/index.html) NICT（独立行政法人 情報通信研究機構）が開発した多言語翻訳エンジンが使われています。\n",
    "\n",
    "### 対話システム\n",
    "音声または文字で入力した文章を、コンピューターが理解して応答を返すシステム。スマートスピーカーやSiriなどのアシストサービス、LINEのボット（自動応答システム）などがある。また、対話システムを容易に構築できるAPIも提供されている。\n",
    "\n",
    "事例\n",
    "\n",
    "- [Amazon Echo](https://www.amazon.co.jp/gp/product/B071ZF5KCM/ref=sv_devicesubnav_0)\n",
    "- [LINE Clova WAVE](https://clova.line.me/)\n",
    "- [Google Home](https://store.google.com/product/google_home?hl=ja)\n",
    "- [Siri](https://www.apple.com/siri/)\n",
    "- [Cortana](https://support.microsoft.com/ja-jp/help/17214/windows-10-what-is)\n",
    "- [りんな](https://www.rinna.jp/) Microsoftが作成したLINEの自動応答システム\n",
    "- [Watson Conversation](https://www.ibm.com/watson/jp-ja/developercloud/conversation.html) 対話システムAPI\n",
    "- [Palro](https://palro.jp/) コミュニケーションロボット\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 なぜ自然言語をベクトルの形にするのか\n",
    "### one-hot表現\n",
    "単語をベクトルで表現する方法としてまず考えられるのはone-hot表現。one-hot表現というのはある要素のみが1でその他の要素が0であるような表現方法のこと。各次元に 1 か 0 を設定することで「その単語か否か」を表す。\n",
    "\n",
    "例えば、one-hot表現でpythonという単語を表すとしましょう。ここで、単語の集合であるボキャブラリは(nlp, python, word, ruby, one-hot)の5単語とします。そうすると、pythonを表すベクトルは以下のようになる。\n",
    "\n",
    "<img src=\"https://camo.qiitausercontent.com/2f7b221071803d127a73b70cbf8c2478d2fd9c43/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f32393834363332332d633465352d326564612d366665302d3137356335653635616536332e706e67\" width=20%>\n",
    "\n",
    "one-hot表現はシンプルだが、ベクトル間の演算で何も意味のある結果を得られないという弱点がある。例えば、単語間で類似度を計算するために内積を取るとしよう。one-hot表現では異なる単語は別の箇所に1が立っていてその他の要素は0なので、異なる単語間の内積を取った結果は0になってしまう。これは望ましい結果とは言えない。また、1単語に1次元を割り当てるので、ボキャブラリ数が増えると非常に高次元になってしまう。\n",
    "\n",
    "### 分散表現\n",
    "それに対して分散表現は、単語を低次元の実数値ベクトルで表す表現。おおよそ50次元から300次元くらいで表現することが多い。例えば、先ほど挙げた単語を分散表現で表すと以下のように表せる。\n",
    "\n",
    "<img src=\"https://camo.qiitausercontent.com/7f8f3d7aaf3e84312762705dd9a3310559994f76/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f37373037392f61646339346330642d333835302d356263332d623136652d3939383636353137383938322e706e67\" width=30%>\n",
    "\n",
    "\n",
    "分散表現を使うことでone-hot表現が抱えていた問題を解決できる。例えば、ベクトル間の演算で単語間の類似度を計算することができるようになりる。上のベクトルを見ると、python と ruby の類似度は python と word の類似度よりも高くなる。また、ボキャブラリ数が増えても各単語の次元数を増やさずに済む。以上のように自然言語をベクトルの形にすることで、単語間の類似度の算出やニューラルネットが計算できるからである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 分布仮説とは何か\n",
    "単語をベクトルで表すアイデアの１つとして「単語の意味は、周囲の単語によって形成される」というものがある。これは、分布仮説 （distributional hypothesis）と呼ばれる。単語の意味は、周囲の単語によって形成される。<br>\n",
    "\n",
    "単語自体には意味がなく、その単語の「コンテキスト（文 脈）」によって、単語の意味が形成される。確かに、意味的に同じ単語 は、同じような文脈で多く出現する。たとえば、「I drink beer.」「We drink wine.」 のように drink の近くには飲み物が表れやすい。また、「I guzzle beer.」 「We guzzle wine.」のような文章があれば、guzzle という単語が drink と同じよう な文脈で使われるということが分かる。そして、guzzle と drink が近い意味の 単語だということも導ける。\n",
    "「コンテキスト」とは、ある中央の単語に対して、その周囲 にある単語を指す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.4 word2vecにはskip-gramとCBOWの2つの方法があるがどのようなものか。どう選択すれば良いのか\n",
    "### CBoW\n",
    "CBoWは、Continuous Bag-of-Wordsの略である。CBoWにおいて、文法と意味を学習していくことは、文脈中の単語から対象単語が現れる条件付き確率を最大化することである。つまり、前後の単語から対象単語を推測していくことになる。\n",
    "\n",
    "    ひとたびフルスピードで回り始めたなら、それを動かし続けるのに努力は必要ない。\n",
    "\n",
    "という文を考えてみよう。 「続ける」を対象単語とすると入力層は、周辺5単語の\n",
    "\n",
    "[ なら,、,それ ,を, 動かし, の, に, 努力, は, 必要]\n",
    "\n",
    "の単語リストそれぞれのone-hotベクトルを入力として、真ん中の「続ける」が来る確率を最大にするように学習させる。\n",
    "\n",
    "<img src=\"https://deepage.net/img/fast_text_facebook/cbow.jpg\" width=30%>\n",
    "\n",
    "そこで、このような1つの隠れ層を持つニューラルネットワークを構築すると$W_{2}$の重み行列がスコアを計算するためのモデルとなる。\n",
    "\n",
    "### Skip-gram\n",
    "Skip-gramはCBoWの逆。単語からその周辺単語を予測する。Skip-gramにとって意味・文法の獲得は、出力層における周辺単語予測のエラー率の合計を最小化することである。先ほどの例だと、「続ける」から[ なら,、,それ ,を, 動かし, の, に, 努力, は, 必要]を予測すること。\n",
    "\n",
    "<img src=\"https://deepage.net/img/fast_text_facebook/skip-gram.jpg\" width=30%>\n",
    "\n",
    "Skip-gramは学習データが少なくてもある程度の精度がでるとされている。 そして、CBoWよりもSkip-gramの方が実験では意味的な精度が高くなり、構文の精度ではあまり大差はない。\n",
    "\n",
    "### どう選択すれば良いのか\n",
    "単語の分散表現の精度の点において、多くの場合、 spkip-gram モデルの方が良い結果が得られる。特に、コーパスが大規模になるにつれて、低頻出の単語や類進問題の性能の点において、skip-gram モデルの方が良い結果が得られる傾向にある。なお、学習速度の点では、CBOW モデルの方が skip-gram モデルよりも高速である。これは、skip-gram モデルの場合は、コンテキストの数だけ損失を求めるため、その計算コストが大きくなることに原因がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## カウントベースと呼ばれる手法もあるがどういったものか。利用する上でどのような違いがあるか\n",
    "### カウントベースの手法\n",
    "「単語の意味は、周囲の単語によって形成される」という仮説に基づき、着目した単語の周囲の単語の頻度をカウントし、集計する手法。\n",
    "\n",
    "<img src=https://cdn-ak.f.st-hatena.com/images/fotolife/s/stillalive0304/20180731/20180731185232.png width=40%>\n",
    "\n",
    "### カウントベース(SVD)の手法の問題点\n",
    "大規模なコーパスを使う場合、語彙数*語彙数の巨大な行列を作る必要がある。計算量的に現実的でない。\n",
    "\n",
    "### 推論ベース(Word2Vec：ニューラルネットワークによる推論)の利点\n",
    "\n",
    "- 推論ベースでは、少量（ミニバッチ）ごとに重みを繰り返し更新して学習が可能。（逐次学習が可能）\n",
    "- 推論ベースでは、単語の意味を的確にとらえたベクトル表現＝単語の「分散表現」を可能にする。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vecなどの手法に入力する前にどのような前処理があるか。英語の場合と日本語の場合を考える\n",
    "### 英語の場合 <br>\n",
    "    1. 単語の分割\n",
    "    2. 大文字を小文字にする\n",
    "    3. One-hot\n",
    "### 日本語の場合\n",
    "    1. 形態素分割（単語分割）\n",
    "    2. 大文字を小文字にする\n",
    "    3. One-hot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
