{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint14 Faster R-CNNの論文読解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 対象論文: \n",
    "## Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 物体検出の分野にはどういった手法が存在したか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "引用1：\"Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal  computation  as  a  bottleneck.  \"<br>\n",
    "\n",
    "引用2:\"Recent  advances  in  object  detection  are  driven  by the   success   of   region  proposal   methods   ( e.g.,   [4]) and  region-based  convolutional  neural  networks  (R-CNNs)  [5].  \"<br>\n",
    "\n",
    "手法：SPPnet、Fast R-CNN、PASCAL、 NYUDv2、SIFT Flow\n",
    "ネットワーク：R-CNNs\n",
    "\n",
    "出典：\"Fully Convolutional Networks for Semantic Segmentation\"\n",
    "引用：\"We then define a skip architecture that combines semantic information from a deep,  coarse layer with appearance information from a shallow,  fine  layer  to  produce  accurate  and  detailed  seg-mentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative im-provement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Fasterとあるが、どういった仕組みで高速化したのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習モジュールに使用するフューチャーマップを共通化することにより、高速化を実現をした。\n",
    "\n",
    "引用：\"Our object detection system, called Faster R-CNN, is composed of two modules. The first module is a deep fully  convolutional  network  that  proposes  regions, and the second module is the Fast R-CNN detector [2] that uses the proposed regions. \"\n",
    "\n",
    "Faster R-CNNと呼ばれる物体検出システムは、2つのモジュールで構成されている。第1のモジュールは、領域を提案するディープな全結合層の畳み込みネットワークであり、第2のモジュールは、提案された領域を使用するFast R-CNN検出器である。\n",
    "\n",
    "引用：\"We therefore need to develop a technique that allows  for  haring  convolutional  layers  between  the two networks, rather than learning two separate net-works. \"\n",
    "\n",
    "したがって、2つの別々のネットワークを学習するのではなく、2つのネットワーク間に畳み込みレイヤを配置することを可能にする技術を開発する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) One-Stageの手法とTwo-Stageの手法はどう違うのか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Stageの手法:特定のクラス検出(物体候補の抽出) <br>\n",
    "Two-Stageの手法は特定のクラスに限らない領域検出と特定のクラスの２段検出(物体認識)\n",
    "\n",
    "引用：\"OverFeat is a one-stage, class-specific detection pipeline, and ours is a two-stage cascade consisting of class-agnostic pro-posals and class-specific detections.\n",
    "In OverFeat, the region-wise features come from a sliding window of one aspect ratio over a scale pyramid. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) RPNとは何か。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力画像から各オブジェクトかどうかの評価スコアと矩形を提案する。\n",
    "\n",
    "\n",
    "引用：\"A  Region  Proposal  Network  (RPN)  takes  an  image (of any size) as input and outputs a set of rectangular object  proposals,  each  with  an  objectness  score.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) RoIプーリングとは何か。\n",
    "\n",
    "RoIプーリング層は、Max プーリングを使用して、入力画像の物体検出領域から抜き出し、該当箇所のフィーチャーマップに変換する。\n",
    "\n",
    "出典：R. Girshick, “Fast R-CNN,” in IEEE International Conference on\n",
    "Computer Vision (ICCV), 2015<br>\n",
    "引用：\"The RoI pooling layer uses max pooling to convert the features inside any valid region of interest into a small fea-ture map with a fixed spatial extent of H × W (e.g., 7 × 7), where H and W are layer hyper-parameters that are inde-pendent of any particular RoI.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) Anchorのサイズはどうするのが適切か。\n",
    "\n",
    "アンカーは、(128、256、512) pixelのボックス領域と(1：1,1：2,2：3)の3つのアスペクト比の3つのスケールを使用。 \"\n",
    "\n",
    "引用：\"Even such a large stride provides good results, though accuracy may befurther improved with a smaller stride.\n",
    "For anchors, we use 3 scales with box areas of 128, 256 , and 512 pixels, and 3 aspect ratios of 1:1, 1:2,and 2:1. \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 何というデータセットを使い、先行研究に比べどういった指標値が得られているか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセット： Microsoft COCO\n",
    "- このデータセットには80個のオブジェクトカテゴリが含まれている。 トレーニングセットの80k画像、検証セットの40k画像、test-devセットの20k画像を試している。\n",
    "\n",
    "評価方法：検出平均平均精度（map）\n",
    "\n",
    "結果：COCOのトレーニングセットを使用して、COASTテストデベロッパーセットでは、Faster R-CNNが42.1％mAP@0.5、mAP @ [.5、.95]が21.5％である。 これは、Fast R-CNN よりもmAP@0.5で2.8％高く、mAP @ [.5、.95]が2.2％高い。   \n",
    "   \n",
    "引用：\"We present more results on the Microsoft COCO object detection dataset [12]. This dataset involves 80 object categories. We experiment with the 80k images on the training set, 40k images on the validation set, and 20k images on the test-dev set. We evaluate the mAP averaged for IoU ∈ [0.5 : 0.05 : 0.95] (COCO’s standard metric, simply denoted as mAP@[.5, .95]) and mAP@0.5 (PASCAL VOC’s metric).\"\n",
    "\n",
    "\"Using the COCO training set to train, Faster R-CNN has 42.1% mAP@0.5 and 21.5% mAP@[.5, .95] on the COCO test-dev set. This is 2.8% higher for mAP@0.5 and 2.2% higher for mAP@[.5, .95] than the Fast R-CNN counterpart under the same protocol (Table 11).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
