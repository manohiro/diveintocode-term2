{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 3s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 172s 7ms/step - loss: 0.4588 - acc: 0.7830 - val_loss: 0.4140 - val_acc: 0.8108\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 166s 7ms/step - loss: 0.3015 - acc: 0.8780 - val_loss: 0.3732 - val_acc: 0.8367\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 165s 7ms/step - loss: 0.2190 - acc: 0.9153 - val_loss: 0.4173 - val_acc: 0.8231\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 170s 7ms/step - loss: 0.1587 - acc: 0.9396 - val_loss: 0.4451 - val_acc: 0.8198\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 167s 7ms/step - loss: 0.1111 - acc: 0.9600 - val_loss: 0.5874 - val_acc: 0.8179\n",
      "25000/25000 [==============================] - 43s 2ms/step\n",
      "Test score: 0.5874459129095078\n",
      "Test accuracy: 0.81792\n"
     ]
    }
   ],
   "source": [
    "'''Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "# Notes\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.6913 - acc: 0.5342 - val_loss: 0.6641 - val_acc: 0.5936\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.6275 - acc: 0.6368 - val_loss: 0.6167 - val_acc: 0.6400\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.5170 - acc: 0.7450 - val_loss: 0.4985 - val_acc: 0.7691\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4503 - acc: 0.7967 - val_loss: 0.4956 - val_acc: 0.7791\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4116 - acc: 0.8195 - val_loss: 0.5208 - val_acc: 0.7571\n",
      "25000/25000 [==============================] - 11s 445us/step\n",
      "Test score: 0.5208023117828369\n",
      "Test accuracy: 0.75712\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(SimpleRNN(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 128s 5ms/step - loss: 0.4738 - acc: 0.7709 - val_loss: 0.3835 - val_acc: 0.8274\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 129s 5ms/step - loss: 0.2880 - acc: 0.8835 - val_loss: 0.3771 - val_acc: 0.8428\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.1789 - acc: 0.9330 - val_loss: 0.4421 - val_acc: 0.8338\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 130s 5ms/step - loss: 0.1063 - acc: 0.9623 - val_loss: 0.5183 - val_acc: 0.8179\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.0647 - acc: 0.9781 - val_loss: 0.6338 - val_acc: 0.8212\n",
      "25000/25000 [==============================] - 33s 1ms/step\n",
      "Test score: 0.6337903826999665\n",
      "Test accuracy: 0.8212\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import GRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvLSTM2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 950 samples, validate on 50 samples\n",
      "Epoch 1/5\n",
      "950/950 [==============================] - 41s 43ms/step - loss: 0.2708 - val_loss: 0.0641\n",
      "Epoch 2/5\n",
      "950/950 [==============================] - 33s 35ms/step - loss: 0.0269 - val_loss: 0.0123\n",
      "Epoch 3/5\n",
      "950/950 [==============================] - 33s 35ms/step - loss: 0.0043 - val_loss: 0.0034\n",
      "Epoch 4/5\n",
      "950/950 [==============================] - 33s 35ms/step - loss: 0.0013 - val_loss: 9.4860e-04\n",
      "Epoch 5/5\n",
      "950/950 [==============================] - 33s 35ms/step - loss: 7.5464e-04 - val_loss: 5.8110e-04\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This script demonstrates the use of a convolutional LSTM network.\n",
    "This network is used to predict the next frame of an artificially\n",
    "generated movie which contains moving squares.\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "\n",
    "# We create a layer which take as input movies of shape\n",
    "# (n_frames, width, height, channels) and returns a movie\n",
    "# of identical shape.\n",
    "\n",
    "seq = Sequential()\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   input_shape=(None, 40, 40, 1),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
    "                   padding='same', return_sequences=True))\n",
    "seq.add(BatchNormalization())\n",
    "\n",
    "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "\n",
    "\n",
    "# Artificial data generation:\n",
    "# Generate movies with 3 to 7 moving squares inside.\n",
    "# The squares are of shape 1x1 or 2x2 pixels,\n",
    "# which move linearly over time.\n",
    "# For convenience we first create movies with bigger width and height (80x80)\n",
    "# and at the end we select a 40x40 window.\n",
    "\n",
    "def generate_movies(n_samples=1200, n_frames=15):\n",
    "    row = 80\n",
    "    col = 80\n",
    "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
    "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
    "                              dtype=np.float)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Add 3 to 7 moving squares\n",
    "        n = np.random.randint(3, 8)\n",
    "\n",
    "        for j in range(n):\n",
    "            # Initial position\n",
    "            xstart = np.random.randint(20, 60)\n",
    "            ystart = np.random.randint(20, 60)\n",
    "            # Direction of motion\n",
    "            directionx = np.random.randint(0, 3) - 1\n",
    "            directiony = np.random.randint(0, 3) - 1\n",
    "\n",
    "            # Size of the square\n",
    "            w = np.random.randint(2, 4)\n",
    "\n",
    "            for t in range(n_frames):\n",
    "                x_shift = xstart + directionx * t\n",
    "                y_shift = ystart + directiony * t\n",
    "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
    "                             y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "                # Make it more robust by adding noise.\n",
    "                # The idea is that if during inference,\n",
    "                # the value of the pixel is not exactly one,\n",
    "                # we need to train the network to be robust and still\n",
    "                # consider it as a pixel belonging to a square.\n",
    "                if np.random.randint(0, 2):\n",
    "                    noise_f = (-1)**np.random.randint(0, 2)\n",
    "                    noisy_movies[i, t,\n",
    "                                 x_shift - w - 1: x_shift + w + 1,\n",
    "                                 y_shift - w - 1: y_shift + w + 1,\n",
    "                                 0] += noise_f * 0.1\n",
    "\n",
    "                # Shift the ground truth by 1\n",
    "                x_shift = xstart + directionx * (t + 1)\n",
    "                y_shift = ystart + directiony * (t + 1)\n",
    "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
    "                               y_shift - w: y_shift + w, 0] += 1\n",
    "\n",
    "    # Cut to a 40x40 window\n",
    "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
    "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
    "    noisy_movies[noisy_movies >= 1] = 1\n",
    "    shifted_movies[shifted_movies >= 1] = 1\n",
    "    return noisy_movies, shifted_movies\n",
    "\n",
    "# Train the network\n",
    "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
    "seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n",
    "        epochs=5, validation_split=0.05)\n",
    "\n",
    "# Testing the network on one movie\n",
    "# feed it with the first 7 positions and then\n",
    "# predict the new positions\n",
    "which = 1004\n",
    "track = noisy_movies[which][:7, ::, ::, ::]\n",
    "\n",
    "for j in range(16):\n",
    "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
    "    new = new_pos[::, -1, ::, ::, ::]\n",
    "    track = np.concatenate((track, new), axis=0)\n",
    "\n",
    "\n",
    "# And then compare the predictions\n",
    "# to the ground truth\n",
    "track2 = noisy_movies[which][::, ::, ::, ::]\n",
    "for i in range(15):\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "    ax = fig.add_subplot(121)\n",
    "\n",
    "    if i >= 7:\n",
    "        ax.text(1, 3, 'Predictions !', fontsize=20, color='w')\n",
    "    else:\n",
    "        ax.text(1, 3, 'Initial trajectory', fontsize=20)\n",
    "\n",
    "    toplot = track[i, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)\n",
    "    ax = fig.add_subplot(122)\n",
    "    plt.text(1, 3, 'Ground truth', fontsize=20)\n",
    "\n",
    "    toplot = track2[i, ::, ::, 0]\n",
    "    if i >= 2:\n",
    "        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n",
    "\n",
    "    plt.imshow(toplot)\n",
    "plt.savefig('%i_animate.png' % (i + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimpleRNNCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "New stacked LSTM: 30s/epoch on CPU (15pct faster)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.6876 - acc: 0.5409 - val_loss: 0.6708 - val_acc: 0.5528\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.6223 - acc: 0.6451 - val_loss: 0.6315 - val_acc: 0.6222\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.5443 - acc: 0.7186 - val_loss: 0.6657 - val_acc: 0.6307\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4727 - acc: 0.7679 - val_loss: 0.6088 - val_acc: 0.6953\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4063 - acc: 0.8150 - val_loss: 0.6421 - val_acc: 0.7148\n",
      "25000/25000 [==============================] - 11s 450us/step\n",
      "Test score: 0.6421183574485779\n",
      "Test accuracy: 0.7148\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import RNN, SimpleRNNCell\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('New stacked LSTM: 30s/epoch on CPU (15pct faster)')\n",
    "cells = [\n",
    "    SimpleRNNCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "]\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(RNN(cells))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各処理の結果比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|実行時間  |Test score  |Test accuracy  |\n",
    "|---|---|---|---|\n",
    "|LSTM  |2  |3  |\n",
    "|4  |5  |6  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRUCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "New stacked LSTM: 30s/epoch on CPU (15pct faster)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 134s 5ms/step - loss: 0.4775 - acc: 0.7691 - val_loss: 0.4314 - val_acc: 0.8006\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 142s 6ms/step - loss: 0.2909 - acc: 0.8812 - val_loss: 0.3586 - val_acc: 0.8465\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 129s 5ms/step - loss: 0.1826 - acc: 0.9302 - val_loss: 0.4548 - val_acc: 0.8247\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 131s 5ms/step - loss: 0.1088 - acc: 0.9615 - val_loss: 0.5372 - val_acc: 0.8148\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 133s 5ms/step - loss: 0.0681 - acc: 0.9762 - val_loss: 0.6497 - val_acc: 0.8196\n",
      "25000/25000 [==============================] - 31s 1ms/step\n",
      "Test score: 0.6497126778757573\n",
      "Test accuracy: 0.8196\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import RNN, GRUCell\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('New stacked LSTM: 30s/epoch on CPU (15pct faster)')\n",
    "cells = [\n",
    "    GRUCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "]\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(RNN(cells))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "New stacked LSTM: 30s/epoch on CPU (15pct faster)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.4573 - acc: 0.7854 - val_loss: 0.3834 - val_acc: 0.8310\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.3036 - acc: 0.8758 - val_loss: 0.3730 - val_acc: 0.8370\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 170s 7ms/step - loss: 0.2169 - acc: 0.9165 - val_loss: 0.5098 - val_acc: 0.7771\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 169s 7ms/step - loss: 0.1583 - acc: 0.9400 - val_loss: 0.4649 - val_acc: 0.8264\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 170s 7ms/step - loss: 0.1105 - acc: 0.9590 - val_loss: 0.5685 - val_acc: 0.8179\n",
      "25000/25000 [==============================] - 42s 2ms/step\n",
      "Test score: 0.5684661542415619\n",
      "Test accuracy: 0.81792\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import RNN, LSTMCell\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('New stacked LSTM: 30s/epoch on CPU (15pct faster)')\n",
    "cells = [\n",
    "    LSTMCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "]\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(RNN(cells))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackedRNNCells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "New stacked LSTM: 30s/epoch on CPU (15pct faster)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 330s 13ms/step - loss: 0.4524 - acc: 0.7876 - val_loss: 0.3879 - val_acc: 0.8293\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 310s 12ms/step - loss: 0.2929 - acc: 0.8821 - val_loss: 0.3775 - val_acc: 0.8322\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 312s 12ms/step - loss: 0.2153 - acc: 0.9182 - val_loss: 0.4019 - val_acc: 0.8318\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 313s 13ms/step - loss: 0.1564 - acc: 0.9428 - val_loss: 0.4324 - val_acc: 0.8256\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 311s 12ms/step - loss: 0.1090 - acc: 0.9611 - val_loss: 0.5161 - val_acc: 0.8248\n",
      "25000/25000 [==============================] - 77s 3ms/step\n",
      "Test score: 0.5160929471731186\n",
      "Test accuracy: 0.82476\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import RNN, LSTMCell\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('New stacked LSTM: 30s/epoch on CPU (15pct faster)')\n",
    "cells = [\n",
    "    LSTMCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    LSTMCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "]\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(RNN(cells))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CuDNNGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 19s 744us/step - loss: 0.4210 - acc: 0.7992 - val_loss: 0.3436 - val_acc: 0.8478\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 16s 647us/step - loss: 0.2289 - acc: 0.9090 - val_loss: 0.3763 - val_acc: 0.8298\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 16s 635us/step - loss: 0.1208 - acc: 0.9559 - val_loss: 0.4861 - val_acc: 0.8285\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 16s 632us/step - loss: 0.0585 - acc: 0.9801 - val_loss: 0.5551 - val_acc: 0.8285\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 16s 633us/step - loss: 0.0316 - acc: 0.9897 - val_loss: 0.7517 - val_acc: 0.8209\n",
      "25000/25000 [==============================] - 5s 198us/step\n",
      "Test score: 0.7516524196100235\n",
      "Test accuracy: 0.82092\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import CuDNNGRU\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(CuDNNGRU(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CuDNNLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 19s 767us/step - loss: 0.4354 - acc: 0.7949 - val_loss: 0.3924 - val_acc: 0.8274\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 16s 656us/step - loss: 0.2558 - acc: 0.8984 - val_loss: 0.3732 - val_acc: 0.8390\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 16s 636us/step - loss: 0.1590 - acc: 0.9393 - val_loss: 0.4327 - val_acc: 0.8279\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 16s 642us/step - loss: 0.0981 - acc: 0.9642 - val_loss: 0.5334 - val_acc: 0.8214\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 16s 650us/step - loss: 0.0709 - acc: 0.9751 - val_loss: 0.6565 - val_acc: 0.8199\n",
      "25000/25000 [==============================] - 5s 211us/step\n",
      "Test score: 0.656510255368352\n",
      "Test accuracy: 0.81992\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(CuDNNLSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ロイター"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "8982 train sequences\n",
      "2246 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (8982, 80)\n",
      "x_test shape: (2246, 80)\n",
      "New stacked LSTM: 30s/epoch on CPU (15pct faster)\n",
      "Build model...\n",
      "Train...\n",
      "Train on 8982 samples, validate on 2246 samples\n",
      "Epoch 1/20\n",
      "8982/8982 [==============================] - 16s 2ms/step - loss: 2.6442 - acc: 0.3213 - val_loss: 2.4580 - val_acc: 0.3620\n",
      "Epoch 2/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 2.4320 - acc: 0.3517 - val_loss: 2.4306 - val_acc: 0.3620\n",
      "Epoch 3/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 2.4301 - acc: 0.3517 - val_loss: 2.4395 - val_acc: 0.3620\n",
      "Epoch 4/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 2.4245 - acc: 0.3517 - val_loss: 2.4418 - val_acc: 0.3620\n",
      "Epoch 5/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 2.3763 - acc: 0.3536 - val_loss: 2.3305 - val_acc: 0.3776\n",
      "Epoch 6/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 2.1928 - acc: 0.4040 - val_loss: 2.3071 - val_acc: 0.4056\n",
      "Epoch 7/20\n",
      "8982/8982 [==============================] - 15s 2ms/step - loss: 2.0333 - acc: 0.4809 - val_loss: 2.3485 - val_acc: 0.4185\n",
      "Epoch 8/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 1.7997 - acc: 0.5481 - val_loss: 2.3807 - val_acc: 0.4239\n",
      "Epoch 9/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 1.6315 - acc: 0.5880 - val_loss: 2.4383 - val_acc: 0.4261\n",
      "Epoch 10/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 1.4945 - acc: 0.6216 - val_loss: 2.4306 - val_acc: 0.4234\n",
      "Epoch 11/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 1.3957 - acc: 0.6496 - val_loss: 2.5585 - val_acc: 0.4212\n",
      "Epoch 12/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 1.2953 - acc: 0.6730 - val_loss: 2.7272 - val_acc: 0.4145\n",
      "Epoch 13/20\n",
      "8982/8982 [==============================] - 15s 2ms/step - loss: 1.2074 - acc: 0.6902 - val_loss: 2.7613 - val_acc: 0.4127\n",
      "Epoch 14/20\n",
      "8982/8982 [==============================] - 15s 2ms/step - loss: 1.1370 - acc: 0.7064 - val_loss: 2.7281 - val_acc: 0.4292\n",
      "Epoch 15/20\n",
      "8982/8982 [==============================] - 16s 2ms/step - loss: 1.0635 - acc: 0.7259 - val_loss: 2.8586 - val_acc: 0.4069\n",
      "Epoch 16/20\n",
      "8982/8982 [==============================] - 14s 2ms/step - loss: 0.9963 - acc: 0.7390 - val_loss: 2.8338 - val_acc: 0.4230\n",
      "Epoch 17/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.9380 - acc: 0.7553 - val_loss: 3.0141 - val_acc: 0.4132\n",
      "Epoch 18/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8745 - acc: 0.7708 - val_loss: 3.0455 - val_acc: 0.4159\n",
      "Epoch 19/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.8317 - acc: 0.7802 - val_loss: 3.0140 - val_acc: 0.4230\n",
      "Epoch 20/20\n",
      "8982/8982 [==============================] - 13s 1ms/step - loss: 0.7843 - acc: 0.7888 - val_loss: 3.1990 - val_acc: 0.4029\n",
      "2246/2246 [==============================] - 1s 453us/step\n",
      "Test score: 3.198995482146687\n",
      "Test accuracy: 0.40293855748851704\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import RNN, SimpleRNNCell\n",
    "from keras.datasets import reuters\n",
    "from keras.utils import np_utils\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_features)\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('New stacked LSTM: 30s/epoch on CPU (15pct faster)')\n",
    "cells = [\n",
    "    SimpleRNNCell(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "]\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(RNN(cells))\n",
    "model.add(Dense(46, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 各モデルの結果を比較\n",
    "|Model |Train time|Predict time|Test score  |Test accuracy  |\n",
    "|---|---|---|---|---|\n",
    "|SimpleRNN |44s |11s |0.52080 |0.75712 |\n",
    "|SimpleRNNCell |44s |11s |0.64211 |0.7148 |\n",
    "|LSTM |168s |43s |0.58744 |0.81792 |\n",
    "|GRU |130s |33s |0.63379 |0.8212 |\n",
    "|GRUCell |134s |31s |0.64971 |0.8196 |\n",
    "|CuDNNGRU |16s |5s |0.75165 |0.82092 |\n",
    "|CuDNNLSTM |16s |5s |0.65651 |0.81992 |\n",
    "\n",
    "以上の結果より、処理速度が最も早いのは「CuDNNGRU」、「CuDNNLSTM」である。他のモデルよりも早いのはGPUを使用したモデルのため、他よりも圧倒的に処理速度が向上している。精度に関しては「SimpleRNN」以外は同程度の精度になっている。以上の結果より、CPUのみ環境で処理速度を重視するのであれば、「SimpleRNN」。処理速度よりも精度であれば、「LSTM」、「GRU」が最適だと思われる。また、GPU環境であれば、「CuDNNGRU」、「CuDNNLSTM」を使用するのが良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTMやGRUにはゲートと呼ばれるものがあるがこれにはどのような働きがあるか\n",
    "ゲートには output ゲート、forget ゲート、input ゲートの3つある。output ゲートは次の隠れ層への出力を制御する。forget ゲートは記憶セルからの情報をどれだけ消すか(忘れる)を制御する。input ゲートは新しい記憶を記憶セルに保存する際に、どの情報を保存するかを制御する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNN、LSTM、GRUはどのように使い分ければ良いか\n",
    "SimpleRNNはSimpleRNN、LSTM、GRUの三つの中で最もパラメータが数が少ないモデルである。そのため、他の二つよりも計算コストが少なく処理も早い。しかい、長い時系列データを学習すると勾配消失が起きやすいため、大規模データには向かいない。よって、時系列データが短く、処理を早くしたい場合に用いるのが良い。<br>\n",
    "LSTMはSimpleRNNの欠点であった長い時系列データでの勾配消失を改良したモデルである。三つの中では最もモデルの表現力が高い。しかし、パラメータが多いため、処理が遅い。よって、時系列データが長く、モデルの表現力を上げたい場合に用いるのが良い。<br>\n",
    "GRUはLSTMよりパラメータ数を削減したモデル。LSTMの改良モデルのため、長い時系列データにも対応が可能である。また、LSTMに比べパラメータが少ないので計算コストが少なく処理も早い。しかし、パラメータを削減しているため、LSTMと比べて表現力が低い。よって、LSTMほど表現力が必要なく、処理を高速化したい場合に用いるのが良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackedRNNを使うことはどういった効果があるか\n",
    "StackedRNN を使用すると RNNレイヤー深く重ねる(層を深く)できる。層を深くすることで表現力の高いモデルを作る事ができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvLSTMはどういった場面で使われるか\n",
    "LSTMで2階テンソルを扱えるので、動画処理で利用が可能となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
