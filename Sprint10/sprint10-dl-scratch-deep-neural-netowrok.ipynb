{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ログ表示\n",
    "class Log:\n",
    "    # 表示ログレベル\n",
    "    Loglevel = 0\n",
    "    def logLevelset(level):\n",
    "        # 表示ログレベルの設定\n",
    "        Log.Loglevel = level\n",
    "    def log_print(level, std, *variable):\n",
    "        if(Log.Loglevel >= level):\n",
    "            if(len(variable) == 0):\n",
    "                print(std)\n",
    "            else:\n",
    "                for i in range(len(variable)):\n",
    "                    index = std.find(\"}\") + 1\n",
    "                    # 改行なし表示\n",
    "                    print(std[:index].format(variable[i]), end='')\n",
    "                    # 表示した分を削除\n",
    "                    std = std.replace(std[:index], \"\", 1)\n",
    "                # 改行表示\n",
    "                print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log.logLevelset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./House_Prices/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data[[\"GrLivArea\", \"YearBuilt\"]]\n",
    "X.head()\n",
    "Xnp = np.array(X)\n",
    "Xnp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[[\"SalePrice\"]]\n",
    "y.head()\n",
    "ynp = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-251b836c2d27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# self.n_output : 出力層のノード数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# self.sigma : ガウス分布の標準偏差\n",
    "# self.lr : 学習率\n",
    "# self.n_nodes1 : 1層目のノード数\n",
    "# self.n_nodes2 : 2層目のノード数\n",
    "# self.n_output : 出力層のノード数\n",
    "\n",
    "optimizer = SGD(self.lr)\n",
    "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation1 = Tanh()\n",
    "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation2 = Tanh()\n",
    "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "self.activation3 = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FC1.B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.W.shape(2, 10)\n",
      "self.B.shape(1, 10)\n",
      "self.W.shape(10, 5)\n",
      "self.B.shape(1, 5)\n",
      "self.W.shape(5, 1)\n",
      "self.B.shape(1, 1)\n"
     ]
    }
   ],
   "source": [
    "n_features = 2\n",
    "n_nodes1 = 10\n",
    "n_nodes2 = 5\n",
    "n_output = 1\n",
    "sigma = 1e-4\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = SGD(lr)\n",
    "FC1 = FC(n_features, n_nodes1, SimpleInitializer(sigma), optimizer)\n",
    "activation1 = Tanh()\n",
    "FC2 = FC(n_nodes1, n_nodes2, SimpleInitializer(sigma), optimizer)\n",
    "activation2 = Tanh()\n",
    "FC3 = FC(n_nodes2, n_output, SimpleInitializer(sigma), optimizer)\n",
    "activation3 = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.forward_x.shape(1460, 2)\n",
      "A.shape(1460, 10)\n",
      "self.forward_x.shape(1460, 10)\n",
      "A.shape(1460, 5)\n",
      "self.forward_x.shape(1460, 5)\n",
      "A.shape(1460, 1)\n"
     ]
    }
   ],
   "source": [
    "A1 = FC1.forward(Xnp)\n",
    "Z1 = activation1.forward(A1)\n",
    "A2 = FC2.forward(Z1)\n",
    "Z2 = activation2.forward(A2)\n",
    "A3 = FC3.forward(Z2)\n",
    "Z3 = activation3.forward(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA3.shape(1460, 1)\n",
      "更新前\n",
      "self.B(1, 1)\n",
      "self.W(5, 1)\n",
      "self.dA(1460, 1)\n",
      "self.dZ(1460, 5)\n",
      "self.dZ.shape(1460, 5)\n",
      "layer.W.shape(5, 1)\n",
      "layer.B.shape(1, 1)\n",
      "layer.W.shape(5, 1)\n",
      "layer.B.shape(1460, 1)\n",
      "更新後\n",
      "self.B(1460, 1)\n",
      "self.W(5, 1)\n",
      "dZ2.shape(1460, 5)\n",
      "dA.shape(1460, 5)\n",
      "dA2.shape(1460, 5)\n",
      "更新前\n",
      "self.B(1, 5)\n",
      "self.W(10, 5)\n",
      "self.dA(1460, 5)\n",
      "self.dZ(1460, 10)\n",
      "self.dZ.shape(1460, 10)\n",
      "layer.W.shape(10, 5)\n",
      "layer.B.shape(1, 5)\n",
      "layer.W.shape(10, 5)\n",
      "layer.B.shape(1460, 5)\n",
      "更新後\n",
      "self.B(1460, 5)\n",
      "self.W(10, 5)\n",
      "dA.shape(1460, 10)\n",
      "更新前\n",
      "self.B(1, 10)\n",
      "self.W(2, 10)\n",
      "self.dA(1460, 10)\n",
      "self.dZ(1460, 2)\n",
      "self.dZ.shape(1460, 2)\n",
      "layer.W.shape(2, 10)\n",
      "layer.B.shape(1, 10)\n",
      "layer.W.shape(2, 10)\n",
      "layer.B.shape(1460, 10)\n",
      "更新後\n",
      "self.B(1460, 10)\n",
      "self.W(2, 10)\n"
     ]
    }
   ],
   "source": [
    "dA3 = activation3.backward(Z3, ynp) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "dZ2 = FC3.backward(dA3)\n",
    "dA2 = activation2.backward(dZ2)\n",
    "dZ1 = FC2.backward(dA2)\n",
    "dA1 = activation1.backward(dZ1)\n",
    "dZ0 = FC1.backward(dA1) # dZ0は使用しない"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.スクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchDeepNeuralNetrowkRegressor:\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=True, lr=1e-4, num_epoch=2000, batch_size=10, sigma=1e-4, n_nodes1=10, n_nodes2=5, n_output=1):\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.num_epoch = num_epoch # epoch回数\n",
    "        self.batch_size = batch_size # バッチサイズ\n",
    "        self.sigma = sigma\n",
    "        self.n_features = 0\n",
    "        self.n_nodes1 = n_nodes1 # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2 # 2層目のノード数\n",
    "        self.n_output = n_output # 出力数（3層目のノード数）\n",
    "        self.cost = []\n",
    "        \n",
    "        # レイヤーのインスタンス\n",
    "        self.FC1 = None\n",
    "        self.FC2 = None\n",
    "        self.FC3 = None\n",
    "        self.activation1 = None\n",
    "        self.activation2 = None\n",
    "        self.activation3 = None\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        if self.verbose:\n",
    "            #verboseをTrueにした際は学習過程などを出力する\n",
    "            print()\n",
    "        \n",
    "        # array変換\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # ミニバッチ生成\n",
    "        get_mini_batch = GetMiniBatch(X, y, batch_size=10)\n",
    "                \n",
    "        \"\"\"\n",
    "        初期化処理\n",
    "        \"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation1 = Tanh()\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation2 = Tanh()\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
    "        self.activation3 = Identity()\n",
    "        \n",
    "        # エポック数分繰り返す\n",
    "        for epoch in range(self.num_epoch):\n",
    "            #print(epoch)\n",
    "            for i, (mini_X, mini_y) in enumerate(get_mini_batch):\n",
    "                \"\"\"\n",
    "                フォワードプロパゲーション\n",
    "                \"\"\"\n",
    "                A1 = self.FC1.forward(X)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                \n",
    "                \"\"\"\n",
    "                フォワードプロパゲーション\n",
    "                \"\"\"\n",
    "                dA3 = self.activation3.backward(Z3, y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "                \n",
    "            # 平均二乗誤差\n",
    "            loss = MSE.calculation(dA3)\n",
    "            self.cost.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # array変換\n",
    "        X = np.array(X)\n",
    "        \n",
    "        get_mini_batch = GetMiniBatch(X, X, batch_size=10)\n",
    "        \n",
    "        for i, (mini_X, mini_y) in enumerate(get_mini_batch):\n",
    "            A1 = self.FC1.forward(X)\n",
    "            Z1 = self.activation1.forward(A1)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            Z2 = self.activation2.forward(A2)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            Z3 = self.activation3.forward(A3)\n",
    "            \n",
    "        return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        Log.log_print(1, \"self.W.shape{}\", self.W.shape)\n",
    "        Log.log_print(2, \"self.W{}\", self.W)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        Log.log_print(1, \"self.B.shape{}\", self.B.shape)\n",
    "        Log.log_print(2, \"self.B{}\", self.B)   \n",
    "        self.forward_x = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        \n",
    "        # AddGrad用\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.forward_x = X.copy()\n",
    "        Log.log_print(2, \"self.forward_x{}\", self.forward_x)\n",
    "        Log.log_print(1, \"self.forward_x.shape{}\", self.forward_x.shape)\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        Log.log_print(1, \"A.shape{}\", A.shape)\n",
    "        Log.log_print(2, \"A{}\", A)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        Log.log_print(1, \"更新前\")\n",
    "        Log.log_print(1, \"self.B{}\", self.B.shape)\n",
    "        Log.log_print(1, \"self.W{}\", self.W.shape)\n",
    "        \n",
    "        self.dA = dA\n",
    "        Log.log_print(1, \"self.dA.shape{}\", self.dA.shape)\n",
    "        \n",
    "        self.dZ = np.dot(dA, self.W.T)\n",
    "        Log.log_print(1, \"self.dZ.shape{}\", self.dZ.shape)\n",
    "        \n",
    "        Log.log_print(2, \"self.dZ{}\", self.dZ)\n",
    "        Log.log_print(1, \"self.dZ.shape{}\", self.dZ.shape)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        \n",
    "        Log.log_print(1, \"更新後\")\n",
    "        Log.log_print(1, \"self.B{}\", self.B.shape)\n",
    "        Log.log_print(1, \"self.W{}\", self.W.shape)\n",
    "        \n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        \n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        \n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    ザビエル分布による初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : floatz\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He分布による初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - self.lr*np.dot(layer.forward_x.T, layer.dA) / layer.dA.shape[0]\n",
    "        layer.B = layer.B - self.lr*np.mean(layer.dA, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        Log.log_print(1, \"layer.W.shape{}\", layer.W.shape)\n",
    "        Log.log_print(2, \"layer.W{}\", layer.W)\n",
    "        Log.log_print(1, \"layer.B.shape{}\", layer.B.shape)\n",
    "        Log.log_print(2, \"layer.B{}\", layer.B)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGrand:\n",
    "    \"\"\"\n",
    "    最適劣勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.HW = layer.HW + layer.W**2\n",
    "        layer.HB = layer.HB + np.mean(layer.B**2, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(1/np.sqrt(layer.HW))*np.dot(layer.forward_x.T, layer.dA) / layer.dA.shape[0]\n",
    "        layer.B = layer.B - self.lr*(1/np.sqrt(layer.HB))*np.mean(layer.dA, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        Log.log_print(1, \"layer.W.shape{}\", layer.W.shape)\n",
    "        Log.log_print(2, \"layer.W{}\", layer.W)\n",
    "        Log.log_print(1, \"layer.B.shape{}\", layer.B.shape)\n",
    "        Log.log_print(2, \"layer.B{}\", layer.B)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ *(1 - np.tanh(self.A)**2)\n",
    "        Log.log_print(1, \"dA.shape{}\", dA.shape)\n",
    "        Log.log_print(2, \"dA{}\", dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(A, 0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A = self.A.copy()\n",
    "        A[A>0]=1\n",
    "        A[A<=0]=0\n",
    "        dA = dZ *A\n",
    "        Log.log_print(1, \"dA.shape{}\", dA.shape)\n",
    "        Log.log_print(2, \"dA{}\", dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恒等関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, A):\n",
    "        return A\n",
    "    def backward(self, Yhat, Y):\n",
    "        return Yhat - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @classmethod\n",
    "    def calculation(self, loss):\n",
    "        return np.mean((loss)**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.説明\n",
    "## 2.1ドロップアウトとは何か\n",
    "Dropoutは、階層の深いニューラルネットを精度よく最適化するためにHintonらによって提案された手法である。<br>\n",
    "Dropoutでは、ニューラルネットワークを学習する際に、ある更新で層の中のノードのうちのいくつかを無効にして（そもそも存在しないかのように扱って）学習を行い、次の更新では別のノードを無効にして学習を行うことを繰り返す。これにより学習時にネットワークの自由度を強制的に小さくして汎化性能を上げ、過学習を避けることができる。隠れ層においては、一般的に50%程度を無効すると良いと言われている。当初Dropoutは全結合のみに適用されていたが、論文によれば、畳み込み層等に適用しても同様に性能を向上させることが確かめられている。\n",
    "![](http://cdn-ak.f.st-hatena.com/images/fotolife/s/sonickun/20160714/20160714170858.jpg)\n",
    "Dropoutが高性能である理由は、「アンサンブル学習」という方法の近似になるからとも言われている。アンサンブル学習とは、複数の機械学習結果を利用して判定を行うことで、学習の性能を上げることである。これを応用した学習器として、ランダムサンプリングしたデータによって学習した多数の決定木を平均化するランダムフォレストなどがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 近年ReLUが一般的に使われている理由\n",
    "ReLU関数は非常にシンプルで、入力が0を超えていれば入力をそのまま出力し、0未満であれば0を返えす。そのため、現代のニューラルネットワークにおいては、一般的にReLU関数が活性化関数として使われている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 重みの初期化方法について\n",
    "### Xavierの初期値\n",
    "「Xavierの初期値」は層のノードの数によって、作用させる係数を変化させる。<br>\n",
    "たとえば、前層から渡されるノード数がn個であるときには、標準偏差√nで割る。<br>\n",
    "つまり以下の値を作用させる。<br>\n",
    "$\\frac{1}{\\sqrt n}$\n",
    "つまり、初期値のバラツキについては、各層ごとにノードの数で均一化しているイメージになる。\n",
    "### Heの初期値\n",
    "Xavierの初期値と共によく使われる初期値として「Heの初期値」がある。<br>\n",
    "作用させる値はXavierと似ているが、標準偏差√(n/2)で割る。\n",
    "つまり以下の値を作用させる。<br>\n",
    "$\\sqrt\\frac{2}{n}$\n",
    "これまで、活性化関数はSigmoid関数を用いていた、Heの初期値は活性化関数は、ReLU関数と一緒に使用する。<br>\n",
    "ReLU関数は、以下のようなシンプルな関数である。<br>\n",
    "$f(x)=max(0,x)$\n",
    "つまり、xが0以下の値のときは0, xが正の値の場合はそのままxの値をとる関数である。\n",
    "![](https://camo.qiitausercontent.com/659861502f9ef615a8344546706e2f4ab7d95d73/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3133343435332f38373338623735632d373833342d636264632d333435392d6464636664363063636338652e706e67)\n",
    "Sigmoid関数の形を見ると、(x,y) = (0, 0.5)を中心に点対称のグラフである。それに比べて、ReLu関数は負の数は0で丸められ、同時に負の領域では値のバラツキは0になる。Heの初期値がReLU関数を採用し、値のバラツキの分散値を半分の値にしているのも、活性化関数の形から直感的にわかる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 代表的な最適化手法について\n",
    "### バッチ勾配降下法\n",
    "ただの勾配降下法(別名：バッチ勾配降下法)は、トレーニングデータセット全体に対するパラメータ$\\theta$について、コスト関数の勾配を計算する。\n",
    "$\\theta = \\theta – \\eta \\cdot \\nabla_\\theta J(\\theta)$\n",
    "たった1回の更新を行うのにも全データセットに対して勾配を計算する必要があるので、バッチ勾配降下法はとても処理速度が遅く、メモリに収まらないデータセットに対して処理はできない。またバッチ勾配降下法はオンラインでの更新、つまり新しい例を得ながらモデルを更新することができない。<br>\n",
    "また、どれぐらいの規模で更新を行うかを決定する学習率を用いて、勾配方向にパラメータを更新する。バッチ勾配降下法は凸面の誤差に対して大域解に、非凸面に対し局所解に収束することを保証する。<br>\n",
    "### 確率的勾配降下法\n",
    "確率的勾配降下法（SGD）は対照的に、それぞれの訓練例$x^{(i)}$とラベル$y^{(i)}$に対してパラメータの更新を行う。<br>\n",
    "$\\theta = \\theta – \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$\n",
    "バッチ勾配降下法は大きなデータに対して冗長な計算を行う。というのも各パラメータの更新の前に類似の例の勾配を再計算するからである。SGDは1度につき1回の更新を行い、こうした冗長性を排除した。それにより、通常はより処理速度が速く、オンラインでの学習でも使用できる。<br>\n",
    "また、SGDは頻繁に更新を行うが、図1のように目的関数を激しく変動させる高い可動性がある。\n",
    "![](http://ruder.io/content/images/2016/01/sgd_fluctuation.png)\n",
    "バッチ勾配降下法においてパラメータは設定された谷間の極小値へ収束するが、その一方で、SGDではその変動によって、新たな、より適切な可能性のある局所解へ素早く移動することができる。しかしSGDでは、それを過剰に行い続けてしまうため、最終的には正確な極小値への収束が難しくななる。しかし、徐々に学習率を下げるようにすると、SGDはバッチ勾配降下法と同様の収束挙動を示していた。そしてほぼ確実に、それぞれ非凸最適化と凸最適化のための局所解か大域解へ収束する。<br>\n",
    "### 勾配降下法の最適化アルゴリズム\n",
    "### Momentum(慣性)\n",
    "SGDには谷間をナビゲートする上で問題がある。谷間とはつまり、表面がカーブしているエリアのことで、ある次元では他の次元よりカーブがもっと急峻になっている。これは、局所的最適の周辺ではよくある。このようなシナリオでは、上の図のように局所的最適に向かう谷底周辺ではSDGが遅々として進まず、SGDは谷間の傾斜を横切って行ったり来たりする。\n",
    "![](http://ruder.io/content/images/2015/12/without_momentum.gif \"図2：Momentumが無いSGD\")\n",
    "![](http://ruder.io/content/images/2015/12/with_momentum.gif \"図3：Momentumが有るSGD\")\n",
    "Momentumとは、下の図のように、関連性のある方向へSGDを加速させ振動を抑制する方法である。現在の更新ベクトルに、過去のタイムステップの更新ベクトルを$\\gamma$の割合だけ加えることにより実現する。<br>\n",
    "$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta)$<br>\n",
    "$\\theta = \\theta – v_t$<br>\n",
    "基本的にMomentumを使う場合は、ボールを丘から転がる。ボールは丘を転がり落ちる際にMomentumを累積し、途中で加速して行く（空気抵抗があれば、終速度に至るまで加速する。「空気抵抗がある」とは、つまり$\\gamma < 1$ということである）。同様のことがパラメータの更新でも起こる。つまり、Momentumの項は勾配が同じ方向に向いている次元に向けて増加する。そして、勾配が方向を変える次元に向けての更新を減少させる。結果的に収束が早まり、振動を抑制することができる。<br>\n",
    "### Nesterovの加速勾配降下法\n",
    "しかし丘を転がり落ちるボールは、斜面のなすがままになっているので、非常に不十分な状態と言える。どこに向かっているのか分かる賢いボールなら、丘の斜面が再び上り坂になる前に速度を落とすことができる。<br>\n",
    "\n",
    "Nesterov accelerated gradient（NAG）6は、Momentumの項に対し、こういった予測ができる能力を与える方法である。パラメータ$\\theta$を動かすために、Momentum項$\\gamma v_{t-1}$を使う。そのため、$\\theta – \\gamma v_{t-1}$を計算すると、次のパラメータの位置の近似値（完全な更新には勾配が欠けている）が分かる。パラメータがどこへ向かうのか、おおよそのことが分かる。現在のパラメータ\\(\\theta\\)に関する勾配を計算するのではなく、未来のパラメータの推定位置を計算することで、効果的に予測することができる。<br>\n",
    "$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta – \\gamma v_{t-1} )$<br>\n",
    "$\\theta = \\theta – v_t$<br>\n",
    "もう一度、Momentum項\\(\\gamma\\)の値を約0.9にセットする。Momentumが現在の勾配（図4の小さな青色のベクトル）を最初に計算し、更新された累積勾配（大きな青色のベクトル）の方向に大きく移動する間に、まずNAGは前の累積の勾配（茶色のベクトル）の方向に大きく移動し、勾配を計測し、訂正する（緑色のベクトル）。この先行の更新は速く進み過すぎるのを防ぎ、反応性を向上する。これにより、幾つかのタスクに関してRNNsのパフォーマンスがかなり向上しました。<br>\n",
    "![](http://ruder.io/content/images/2016/01/nesterov_update_vector.png)\n",
    "更新を誤差関数の勾配に適応させ、SGDの速度を上げることができるようになった。個々のパラメータの更新についても、その重要性に応じて更新をより大きくしたり小さくしたりして実行するよう適応させる。<br>\n",
    "### Adagrad\n",
    "Adagradは以下に示すような、勾配ベースの最適化アルゴリズムである。Adagradは学習率をパラメータに適応させる。そして、まれなパラメータに対してはより大きな更新を、頻出のパラメータに対してはより小さな更新を実行する。このような理由から、スパースなデータを扱うのに適していいる。Deanらが発見したのは、AdagradがSGDの頑強性を大幅に改善し、Googleでの大規模なニューラルネットワークの訓練、中でもYouTubeビデオの中の猫を認識するという訓練の際に使用したということであった。さらに、Penningtonらは、まれな語が頻出な語よりもさらに大きな更新を必要とする時に、GloVeの語の埋め込みの訓練に際してAdagradを使用した。<br>\n",
    "ここまで、それぞれのパラメータ$\\theta_i$が同じ学習率$\\eta$を使うように、全てのパラメータ$\\theta$を一度に更新していた。Adagradが各タイムステップ$t$で、それぞれのパラメータ$\\theta_i$に対して異なる学習率を使うので、ここではまずAdagradのパラメータごとの更新を示したのち、それをベクトル化することにした。簡単化のために、タイムステップ$t$におけるパラメータ$\\theta_i$に関する目的関数の勾配を$g_{t, i}$とする。<br>\n",
    "$g_{t, i} = \\nabla_\\theta J( \\theta_i )$<br>\n",
    "すると、タイムステップ\\(t\\)ごとの、SDGが各パラメータ$\\theta_i$に行う更新は、次のようになる。<br>\n",
    "$\\theta_{t+1, i} = \\theta_{t, i} – \\eta \\cdot g_{t, i}$\n",
    "Adagradの更新規則は、タイムステップ$t$ごとに、$\\theta_i$に対して計算されている過去の勾配に基づいて、各パラメータ$\\theta_i$に対して一般的な学習率$\\eta$を修正する。<br>\n",
    "$\\theta_{t+1, i} = \\theta_{t, i} – \\dfrac{\\eta}{\\sqrt{G_{t, ii} + \\epsilon}} \\cdot g_{t, i}$<br>\n",
    "ここの$G_{t} \\in \\mathbb{R}^{d \\times d}$は対角行列で、それぞれの対角成分$i, i$は、タイムステップ$t$までの、$\\theta_i$に対する勾配の二乗和となる。$\\epsilon$はゼロによる割り算を避ける平滑化項である（たいていは$1e-8$のオーダ）。<br>\n",
    "\n",
    "$G_{t}$は、その対角成分に沿って、全てのパラメータ$\\theta$に関する過去の勾配の二乗和を含んでいるので、$G_{t}$と$g_{t}$の間で要素ごとに加算を行う行列ベクトルの掛け算$\\odot$を実行することによって、以下の様に実装をベクトル化することができる。<br>\n",
    "$\\theta_{t+1} = \\theta_{t} – \\dfrac{\\eta}{\\sqrt{G_{t} + \\epsilon}} \\odot g_{t}$<br>\n",
    "Adagradの主な利点の1つは、手動で学習率を調整することが不要であること。大部分の実装ではデフォルトの0.01かその辺りにとどめた値を使用する。<br>\n",
    "Adagradの主な弱点は、分母の二乗勾配の蓄積である。どの加算項も正の値なので、訓練の間、累積和は増加し続ける。このことは、次々に学習率が低下し、最終的に極めて小さくなる。その時点で、もはやアルゴリズムはさらなる知識を得ることができない。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
