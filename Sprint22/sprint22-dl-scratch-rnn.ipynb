{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ログ表示\n",
    "class Log:\n",
    "    # 表示ログレベル\n",
    "    Loglevel = 0\n",
    "    def logLevelset(level):\n",
    "        # 表示ログレベルの設定\n",
    "        Log.Loglevel = level\n",
    "    def log_print(level, std, *variable):\n",
    "        if(Log.Loglevel >= level):\n",
    "            if(len(variable) == 0):\n",
    "                print(std)\n",
    "            else:\n",
    "                for i in range(len(variable)):\n",
    "                    index = std.find(\"}\") + 1\n",
    "                    # 改行なし表示\n",
    "                    print(std[:index].format(variable[i]), end='')\n",
    "                    # 表示した分を削除\n",
    "                    std = std.replace(std[:index], \"\", 1)\n",
    "                # 改行表示\n",
    "                print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Log.logLevelset(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ読込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_sequences, n_nodes)) # ←変更\n",
    "b = np.array([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプル実装用のテストコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FCのフォワード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Tanh()\n",
    "\n",
    "for batch in range(batch_size):\n",
    "    for i in range(n_sequences):\n",
    "        if(i == 0):\n",
    "            A = np.dot(x[batch, i], w_x) + b\n",
    "        else:\n",
    "            A = np.dot(x[batch, i], w_x) + np.dot(h[batch, i-1], w_h) + b\n",
    "        h[batch,i] = activation.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79494228 0.81839002 0.83939649 0.85584174]\n",
      "[[[0.76188798 0.76213958 0.76239095 0.76255841]\n",
      "  [0.792209   0.8141834  0.83404912 0.84977719]\n",
      "  [0.79494228 0.81839002 0.83939649 0.85584174]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.tanh(A))\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.79494228, 0.81839002, 0.83939649, 0.85584174]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = Tanh()\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    if(i == 0):\n",
    "        A = np.dot(x[:, i], w_x) + b\n",
    "    else:\n",
    "        A = np.dot(x[:, i], w_x) + np.dot(h[:, i-1], w_h) + b\n",
    "    h[:,i] = activation.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.スクラッチ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNClassifier:\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=True, lr=1e-4, num_epoch=2000, batch_size=10, sigma=1e-4, n_nodes1=10, n_nodes2=5, n_output=1):\n",
    "        self.verbose = verbose\n",
    "        self.lr = lr\n",
    "        self.num_epoch = num_epoch # epoch回数\n",
    "        self.sigma = sigma\n",
    "        self.n_features = 0\n",
    "        self.batch_size = 0\n",
    "        self.n_sequences = 0\n",
    "        self.n_nodes1 = n_nodes1 # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2 # 2層目のノード数\n",
    "        self.n_output = n_output # 出力数（3層目のノード数）\n",
    "        self.cost = []\n",
    "        \n",
    "        # レイヤーのインスタンス\n",
    "        self.FC1 = None\n",
    "        self.FC2 = None\n",
    "        self.FC3 = None\n",
    "        self.activation = None\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \n",
    "        # array変換\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "                \n",
    "        \"\"\"\n",
    "        初期化処理\n",
    "        \"\"\"\n",
    "        self.batch_size = X.shape[0]\n",
    "        self.n_sequences = X.shape[1]\n",
    "        self.n_features = X.shape[2]\n",
    "        \n",
    "        optimizer = SGD(self.lr)\n",
    "        self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), Tanh(), optimizer)\n",
    "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), Tanh(), optimizer)\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), Tanh(), optimizer)\n",
    "        self.activation = Tanh()\n",
    "        \n",
    "        # エポック数分繰り返す\n",
    "        for epoch in range(self.num_epoch):\n",
    "            for i in range(self.n_sequences):\n",
    "                \"\"\"\n",
    "                フォワードプロパゲーション\n",
    "                \"\"\"\n",
    "                A1 = self.FC1.forward(X, i)\n",
    "                A2 = self.FC2.forward(A1, i)\n",
    "                A3 = self.FC3.forward(A2, i)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "                Z3 = softmax(Z3)\n",
    "                \n",
    "                \n",
    "                \"\"\"\n",
    "                フォワードプロパゲーション\n",
    "                \"\"\"\n",
    "                dA3 = Z3 - y # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "                \n",
    "            # 平均二乗誤差\n",
    "            loss = MSE.calculation(dA3)\n",
    "            self.cost.append(loss)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # array変換\n",
    "        X = np.array(X)\n",
    "        \n",
    "        batch_size = X.shape[0]\n",
    "        n_sequences = X.shape[1]\n",
    "        n_features = X.shape[2]\n",
    "        prdict = np.zeros(n_sequences)\n",
    "        \n",
    "        for i in range(n_sequences):\n",
    "            A1 = self.FC1.forward(X)\n",
    "            A2 = self.FC2.forward(Z1)\n",
    "            A3 = self.FC3.forward(Z2)\n",
    "            prdict[i] = softmax(A3)\n",
    "            \n",
    "        return prdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全結合層のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, activation, optimizer):\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.W, self.Wh, self.Bを初期化する\n",
    "        self.W, self.Wh, self.h = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2) \n",
    "        self.forward_x = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        \n",
    "        # AddGrad用\n",
    "        self.HW = 0\n",
    "        self.HB = 0\n",
    "        \n",
    "    def forward(self, X, n_sequences):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        n_sequences : 現在のサンプル数\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"            \n",
    "        self.forward_x = X.copy()\n",
    "        \n",
    "        if(n_sequences == 0):\n",
    "            A = np.dot(X[:, n_sequences], self.W) + self.B\n",
    "        else:\n",
    "            A = np.dot(X[:, n_sequences], self.W) + np.dot(self.h[:, n_sequences-1], self.Wh) + self.B\n",
    "        \n",
    "        # Afin\n",
    "        A = self.activation(A)\n",
    "        self.h[:, n_sequences] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA, n_sequences):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        n_sequences : 現在のサンプル数\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dA = dA\n",
    "        \n",
    "        self.dZ = np.dot(dA, self.W.T)\n",
    "        \n",
    "        # 更新\n",
    "        self = self.optimizer.update(self, n_sequences)\n",
    "        \n",
    "        self.dZ = self.activation.backward(self.dZ)\n",
    "        \n",
    "        return self.dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初期化方法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2, n_samples):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        n_samples : int\n",
    "           入力データのサンプル数        \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        \n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        Wh = self.sigma * np.random.randn(n_nodes2, n_nodes1)\n",
    "        h = np.zeros((n_samples, n_nodes2), dtype = float)\n",
    "        \n",
    "        return W, Wh, h\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        \n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    ザビエル分布による初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : floatz\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        Wh = self.sigma * np.random.randn(n_nodes2, n_nodes1)\n",
    "        \n",
    "        return W, Wh\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    He分布による初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    function : std\n",
    "      初期関数のモード\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sigma = 0\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        self.sigma = np.sqrt(2 / n_nodes1)\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        Wh = self.sigma * np.random.randn(n_nodes2, n_nodes1)\n",
    "        \n",
    "        return W, Wh\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        \n",
    "        return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適化手法のクラス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer, n_sequences):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        n_sequences : 現在のサンプル数 \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - np.mean(self.lr*np.dot(layer.forward_x.T, layer.dA), axis=0) / layer.dA.shape[0]\n",
    "        layer.B = layer.B - self.lr*np.mean(layer.dA, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        layer.Wh = layer.Wh - np.mean(self.lr*np.dot(layer.h[:,n_sequences-1].T, layer.dA), axis=0) / layer.dA.shape[0]\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGrand:\n",
    "    \"\"\"\n",
    "    最適劣勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, layer, n_sequences):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        n_sequences : 現在のサンプル数 \n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.HW = layer.HW + layer.W**2\n",
    "        layer.HB = layer.HB + np.mean(layer.B**2, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        layer.W = layer.W - self.lr*(1/np.sqrt(layer.HW))*np.dot(layer.forward_x.T, layer.dA) / layer.dA.shape[0]\n",
    "        layer.B = layer.B - self.lr*(1/np.sqrt(layer.HB))*np.mean(layer.dA, axis=0)[np.newaxis,:]\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.tanh(A)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        dA = dZ *(1 - np.tanh(self.A)**2)\n",
    "        Log.log_print(1, \"dA.shape{}\", dA.shape)\n",
    "        Log.log_print(2, \"dA{}\", dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.A = 0\n",
    "        \n",
    "    def forward(self, A):\n",
    "        self.A = A\n",
    "        return np.maximum(A, 0)\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        A = self.A.copy()\n",
    "        A[A>0]=1\n",
    "        A[A<=0]=0\n",
    "        dA = dZ *A\n",
    "        Log.log_print(1, \"dA.shape{}\", dA.shape)\n",
    "        Log.log_print(2, \"dA{}\", dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恒等関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self, A):\n",
    "        return A\n",
    "    def backward(self, Yhat, Y):\n",
    "        return Yhat - Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ロス関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE:\n",
    "    @classmethod\n",
    "    def calculation(self, loss):\n",
    "        return np.mean((loss)**2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(loss, y):\n",
    "    return - np.mean( np.sum(y.T*np.log(loss), axis=1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DropOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOut:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.mask = np.random.rand(*X.shape) > 0.5\n",
    "        return X*self.mask\n",
    "    \n",
    "    def backward(self, X):\n",
    "        return X*self.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNNが通常の全結合層ニューラルネットワークと異なるのはどういった点か\n",
    "異なるのは時系列データを扱うために、過去の自分自身（同レイヤー）の出力を入力に加算する点である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## なぜRNNは自然言語処理に使われるのか\n",
    "単語は文脈によって意味が変わるので、前後の関係を考慮できるRNNが使用される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SimpleRNNは勾配消失が起きやすいと言われているがそれはなぜか\n",
    "RNNの逆伝搬では時間方向の勾配に着目すると逆伝搬によって伝わる勾配は「tanh」を通過することになる。「tanh」ノードは通過するたびに値はどんどん小さくなるため、時系列データが長いものは勾配消失が起きやすい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
